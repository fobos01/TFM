@article{PIMENTEL2016744,
	title = {Sharp Hessian integrability estimates for nonlinear elliptic equations: An asymptotic approach},
	journal = {Journal de Mathématiques Pures et Appliquées},
	volume = {106},
	number = {4},
	pages = {744-767},
	year = {2016},
	issn = {0021-7824},
	doi = {https://doi.org/10.1016/j.matpur.2016.03.010},
	url = {https://www.sciencedirect.com/science/article/pii/S0021782416300101},
	author = {Edgard A. Pimentel and Eduardo V. Teixeira},
	keywords = {Fully nonlinear elliptic equations, Regularity theory, A priori  estimates},
	abstract = {We establish sharp W2,p regularity estimates for viscosity solutions of fully nonlinear elliptic equations under minimal, asymptotic assumptions on the governing operator F. By means of geometric tangential methods, we show that if the recession of the operator F – formally given by F⁎(M):=∞−1F(∞M) – is convex, then any viscosity solution to the original equation F(D2u)=f(x) is locally of class W2,p, provided f∈Lp, p>d, with appropriate universal estimates. Our result extends to operators with variable coefficients and in this setting they are new even under convexity of the frozen coefficient operator, M↦F(x0,M), as oscillation is measured only at the recession level. The methods further yield BMO regularity of the Hessian, provided the source lies in that space. As a final application, we establish the density of W2,p solutions within the class of all continuous viscosity solutions, for generic fully nonlinear operators F. This result gives an alternative tool for treating common issues often faced in the theory of viscosity solutions.
%	Résumé
%	Dans cet article on établit la régularité W2,p des solutions de viscosité des équations elliptiques complètement nonlinéaires. On utilise une méthode géométrique tangentielle, qui est basée sur la notion de fonction de recession d'un opérateur arbitraire F – formellement donnée par F⁎(M):=∞−1F(∞M). De plus, on examine le problème de l'opérateur avec coefficients variables, et on étudie la régularité des solutions dans l'espace BMO. La dernière partie traite d'un résultat concernant la densité des solutions W2,p dans la classe des solutions de viscosité continue.}
}

@article{da_S_Bessa_2023,
	doi = {10.1016/j.jde.2023.05.006},
	
	url = {https://doi.org/10.1016%2Fj.jde.2023.05.006},
	
	year = 2023,
	month = {sep},
	
	publisher = {Elsevier {BV}
	},
	
	volume = {367},
	
	pages = {451--493},
	
	author = {Junior da S. Bessa and Jo{\~{a}}o Vitor da Silva and Maria N.B. Frederico and Gleydson C. Ricarte},
	
	title = {Sharp Hessian estimates for fully nonlinear elliptic equations under relaxed convexity assumptions, oblique boundary conditions and applications},
	
	journal = {Journal of Differential Equations}
}

@article{navarro_sánchez_2020, title={Sharp estimates of semistable radial solutions of k-Hessian equations}, volume={150}, DOI={10.1017/prm.2019.14}, number={4}, journal={Proceedings of the Royal Society of Edinburgh Section A: Mathematics}, publisher={Royal Society of Edinburgh Scotland Foundation}, author={Navarro, Miguel Angel and Sánchez, Justino}, year={2020}, pages={2083–2115}}
@misc{kagglePredictGenetic,
	author = {Amit Kumar},
	title = {{P}redict the genetic disorders dataset-of genomes --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/datasets/aibuzz/predict-the-genetic-disorders-datasetof-genomes}},
	year = {2021},
	note = {[Accessed 20-08-2025]},
}

@ARTICLE{Sander2000-sm,
  title    = "Genomic medicine and the future of health care",
  author   = "Sander, C",
  abstract = "Genomic technologies and computational advances are leading to an
              information revolution in biology and medicine. Simulations of
              molecular processes in cells and predictions of drug effects in
              humans will advance pharmaceutical research and speed up clinical
              trials. Computational prognostics and diagnostics that combine
              with genotyping and molecular profiling may soon cause
              fundamental changes in the practice of health care.",
  journal  = "Science",
  volume   =  287,
  number   =  5460,
  pages    = "1977--1978",
  month    =  mar,
  year     =  2000,
  address  = "United States",
  language = "en"
}
@Article{Vinkšel2021,
author={Vink{\v{s}}el, Mateja
and Writzl, Karin
and Maver, Ale{\v{s}}
and Peterlin, Borut},
title={Improving diagnostics of rare genetic diseases with NGS approaches},
journal={Journal of Community Genetics},
year={2021},
month={Apr},
day={01},
volume={12},
number={2},
pages={247-256},
abstract={According to a rough estimate, one in fifteen people worldwide is affected by a rare disease. Rare diseases are therefore common in clinical practice; however, timely diagnosis of rare diseases is still challenging. Introduction of novel methods based on next-generation sequencing (NGS) technology offers a successful diagnosis of genetically heterogeneous disorders, even in case of unclear clinical diagnostic hypothesis. However, the application of novel technology differs among the centres and health systems significantly. Our goal is to discuss the impact of the implementation of NGS in the diagnosis of rare diseases and present advantages along with challenges of diagnostic approach. Systematic implementation of NGS in health systems can significantly improve the access of patients with rare diseases to diagnosis and reduce the dependence of national health systems for cross-border collaboration.},
issn={1868-6001},
doi={10.1007/s12687-020-00500-5},
url={https://doi.org/10.1007/s12687-020-00500-5}
}

@ARTICLE{Lee2025-vx,
  title    = "Understanding Genetic Screening: Harnessing Health Information to
              Prevent Disease Risks",
  author   = "Lee, Chung-Lin and Chuang, Chih-Kuang and Chiu, Huei-Ching and
              Chang, Ya-Hui and Tu, Yuan-Rong and Lo, Yun-Ting and Lin,
              Hsiang-Yu and Lin, Shuan-Pei",
  abstract = "Genetic screening analyzes an individual's genetic information to
              assess disease risk and provide personalized health
              recommendations. This article introduces the public to genetic
              screening, explaining its definition, principles, history, and
              common types, including prenatal, newborn, adult disease risk,
              cancer, and pharmacogenetic screening. It elaborates on the
              benefits of genetic screening, such as early risk detection,
              personalized prevention, family risk assessment, and reproductive
              decision-making. The article also notes limitations, including
              result interpretation uncertainty, psychological and ethical
              issues, and privacy and discrimination risks. It provides advice
              on selecting suitable screening, consulting professionals,
              choosing reliable institutions, and understanding screening
              purposes and limitations. Finally, it discusses applying
              screening results through lifestyle adjustments, regular
              check-ups, and preventive treatments. By comprehensively
              introducing genetic screening, the article aims to raise public
              awareness and encourage utilizing this technology to prevent
              disease and maintain health.",
  journal  = "Int J Med Sci",
  volume   =  22,
  number   =  4,
  pages    = "903--919",
  month    =  jan,
  year     =  2025,
  address  = "Australia",
  keywords = "Disease Prevention; Genetic Disorders; Genetic Screening; Health
              Management; Personalized Medicine",
  language = "en"
}

@ARTICLE{Tinker2024-xg,
  title    = "Diagnostic delay in monogenic disease: A scoping review",
  author   = "Tinker, Rory J and Fisher, Miles and Gimeno, Alex F and Gill,
              Kayce and Ivey, Camille and Peterson, Josh F and Bastarache, Lisa",
  abstract = "PURPOSE: Diagnostic delay in monogenic disease is reportedly
              common. We conducted a scoping review investigating variability
              in study design, results, and conclusions. METHODS: We searched
              the academic literature on January 17, 2023, for original peer
              reviewed journals and conference articles that quantified
              diagnostic delay in monogenic disease. We abstracted the reported
              diagnostic delay, relevant study design features, and
              definitions. RESULTS: Our search identified 259 articles
              quantifying diagnostic delay in 111 distinct monogenetic
              diseases. Median reported diagnostic delay for all studies
              collectively in monogenetic diseases was 5.0 years (IQR 2-10).
              There was major variation in the reported delay within individual
              monogenetic diseases. Shorter delay was associated with disorders
              of childhood metabolism, immunity, and development. The majority
              (67.6\%) of articles that studied delay reported an improvement
              with calendar time. Study design and definitions of delay were
              highly heterogenous. Three gaps were identified: (1) no studies
              were conducted in the least developed countries, (2) delay has
              not been studied for the majority of known, or (3) most prevalent
              genetic diseases. CONCLUSION: Heterogenous study design and
              definitions of diagnostic delay inhibit comparison across
              studies. Future efforts should focus on standardizing delay
              measurements, while expanding the research to low-income
              countries.",
  journal  = "Genet Med",
  volume   =  26,
  number   =  4,
  pages    = "101074",
  month    =  jan,
  year     =  2024,
  address  = "United States",
  keywords = "Diagnostic delay; Genetic disease; Informatics; Rare disease;
              Scoping review",
  language = "en"
}
@Article{Alsentzer2025,
author={Alsentzer, Emily
and Li, Michelle M.
and Kobren, Shilpa N.
and Noori, Ayush
and Kohane, Isaac S.
and Zitnik, Marinka
and Network, Undiagnosed Diseases},
title={Few shot learning for phenotype-driven diagnosis of patients with rare genetic diseases},
journal={npj Digital Medicine},
year={2025},
month={Jun},
day={20},
volume={8},
number={1},
pages={380},
abstract={There are over 7000 rare diseases, some affecting 3500 or fewer patients in the United States. Due to clinicians' limited experience with such diseases and the heterogeneity of clinical presentations, {\textasciitilde}70{\%} of individuals seeking a diagnosis remain undiagnosed. Deep learning has demonstrated success in aiding the diagnosis of common diseases. However, existing approaches require labeled datasets with thousands of diagnosed patients per disease. We present SHEPHERD, a few-shot learning approach for multi-faceted rare disease diagnosis. SHEPHERD performs deep learning over a knowledge graph enriched with rare disease information and is trained on a dataset of simulated rare disease patients. We demonstrate SHEPHERD's effectiveness across diverse diagnostic tasks, performing causal gene discovery, retrieving ``patients-like-me'', and characterizing novel disease presentations, using real-world cohorts from the Undiagnosed Diseases Network (N{\thinspace}={\thinspace}465), MyGene2 (N{\thinspace}={\thinspace}146), and the Deciphering Developmental Disorders study (N{\thinspace}={\thinspace}1431). SHEPHERD demonstrates the potential of knowledge-grounded deep learning to accelerate rare disease diagnosis.},
issn={2398-6352},
doi={10.1038/s41746-025-01749-1},
url={https://doi.org/10.1038/s41746-025-01749-1}
}

﻿@Article{Alharbi2022,
author={Alharbi, Wardah S.
and Rashid, Mamoon},
title={A review of deep learning applications in human genomics using next-generation sequencing data},
journal={Human Genomics},
year={2022},
month={Jul},
day={25},
volume={16},
number={1},
pages={26},
abstract={Genomics is advancing towards data-driven science. Through the advent of high-throughput data generating technologies in human genomics, we are overwhelmed with the heap of genomic data. To extract knowledge and pattern out of this genomic data, artificial intelligence especially deep learning methods has been instrumental. In the current review, we address development and application of deep learning methods/models in different subarea of human genomics. We assessed over- and under-charted area of genomics by deep learning techniques. Deep learning algorithms underlying the genomic tools have been discussed briefly in later part of this review. Finally, we discussed briefly about the late application of deep learning tools in genomic. Conclusively, this review is timely for biotechnology or genomic scientists in order to guide them why, when and how to use deep learning methods to analyse human genomic data.},
issn={1479-7364},
doi={10.1186/s40246-022-00396-x},
url={https://doi.org/10.1186/s40246-022-00396-x}
}

@Book{Arshad2023,
author={Arshad, Muhammad A.
and Shahriar, Sakib
and Anjum, Khizar},
title={The Power Of Simplicity: Why Simple Linear Models Outperform Complex Machine Learning Techniques -- Case Of Breast Cancer Diagnosis},
year={2023},
month={Jun},
day={04}
}



@article{Hua2025Clinical,
	author = {Hua, Yuchen and Stead, Thor S. and George, Andrew and Ganti, Latha},
	journal = {Academic Medicine & Surgery},
	doi = {10.62186/001c.131964},
	year = {2025},
	month = {mar 7},
	publisher = {University Medical Press},
	title = {Clinical {Risk} {Prediction} with {Logistic} {Regression}: Best {Practices}, {Validation} {Techniques}, and {Applications} in {Medical} {Research}},
}
﻿@Article{Sadr2025,
author={Sadr, Hossein
and Nazari, Mojdeh
and Khodaverdian, Zeinab
and Farzan, Ramyar
and Yousefzadeh-Chabok, Shahrokh
and Ashoobi, Mohammad Taghi
and Hemmati, Hossein
and Hendi, Amirreza
and Ashraf, Ali
and Pedram, Mir Mohsen
and Hasannejad-Bibalan, Meysam
and Yamaghani, Mohammad Reza},
title={Unveiling the potential of artificial intelligence in revolutionizing disease diagnosis and prediction: a comprehensive review of machine learning and deep learning approaches},
journal={European Journal of Medical Research},
year={2025},
month={May},
day={26},
volume={30},
number={1},
pages={418},
abstract={The rapid advancement of Machine Learning (ML) and Deep Learning (DL) technologies has revolutionized healthcare, particularly in the domains of disease prediction and diagnosis. This study provides a comprehensive review of ML and DL applications across sixteen diverse diseases, synthesizing findings from research conducted between 2015 and 2024. We explore these technologies' methodologies, effectiveness, and clinical outcomes, highlighting their transformative potential in healthcare settings. Although ML and DL demonstrate remarkable accuracy and efficiency in disease prediction and diagnosis, challenges including quality of data, interpretability of models, and their integration into clinical workflows remain significant barriers. By evaluating advanced approaches and their outcomes, this review not only underscores the current capabilities of ML and DL but also identifies key areas for future research. Ultimately, this work aims to serve as a roadmap for advancing healthcare practices, enhancing clinical decision making, and strengthening patient outcomes through the effective and responsible implementation of AI-driven technologies.},
issn={2047-783X},
doi={10.1186/s40001-025-02680-7},
url={https://doi.org/10.1186/s40001-025-02680-7}
}

﻿@Book{Siddik2024,
author={Siddik, Abu
and Badal, Faisal
and Islam, Afroza},
title={Comparative Performance of Machine Learning Algorithms for Early Genetic Disorder and Subclass Classification},
year={2024},
month={Oct},
day={14}
}

@ARTICLE{Li2025-zq,
  title    = "An imaging and genetic-based deep learning network for
              Alzheimer's disease diagnosis",
  author   = "Li, Yuhan and Niu, Donghao and Qi, Keying and Liang, Dong and
              Long, Xiaojing",
  abstract = "Conventional computer-aided diagnostic techniques for Alzheimer's
              disease (AD) predominantly rely on magnetic resonance imaging
              (MRI) in isolation. Genetic imaging methods, by establishing the
              link between genes and brain structures in disease progression,
              facilitate early prediction of AD development. While deep
              learning methods based on MRI have demonstrated promising results
              for early AD diagnosis, the limited dataset size has led most AD
              studies to lean on statistical approaches within the realm of
              imaging genetics. Existing deep-learning approaches typically
              utilize pre-defined regions of interest and risk variants from
              known susceptibility genes, employing relatively straightforward
              feature fusion methods that fail to fully capture the
              relationship between images and genes. To address these
              limitations, we proposed a multi-modal deep learning
              classification network based on MRI and single nucleotide
              polymorphism (SNP) data for AD diagnosis and mild cognitive
              impairment (MCI) progression prediction. Our model leveraged a
              convolutional neural network (CNN) to extract whole-brain
              structural features, a Transformer network to capture genetic
              features, and employed a cross-transformer-based network for
              comprehensive feature fusion. Furthermore, we incorporated an
              attention-map-based interpretability method to analyze and
              elucidate the structural and risk variants associated with AD and
              their interrelationships. The proposed model was trained and
              evaluated using 1,541 subjects from the ADNI database.
              Experimental results underscored the superior performance of our
              model in effectively integrating and leveraging information from
              both modalities, thus enhancing the accuracy of AD diagnosis and
              prediction.",
  journal  = "Front Aging Neurosci",
  volume   =  17,
  pages    = "1532470",
  month    =  mar,
  year     =  2025,
  address  = "Switzerland",
  keywords = "Alzheimer's disease; MRI; SNP; multi-scale deep convolutional
              networks; transformer",
  language = "en"
}


@article{10.1093/bioinformatics/btx150,
    author = {Stricker, Georg and Engelhardt, Alexander and Schulz, Daniel and Schmid, Matthias and Tresch, Achim and Gagneur, Julien},
    title = {GenoGAM: genome-wide generalized additive models for ChIP-Seq analysis},
    journal = {Bioinformatics},
    volume = {33},
    number = {15},
    pages = {2258-2265},
    year = {2017},
    month = {03},
    abstract = {Chromatin immunoprecipitation followed by deep sequencing (ChIP-Seq) is a widely used approach to study protein–DNA interactions. Often, the quantities of interest are the differential occupancies relative to controls, between genetic backgrounds, treatments, or combinations thereof. Current methods for differential occupancy of ChIP-Seq data rely however on binning or sliding window techniques, for which the choice of the window and bin sizes are subjective.Here, we present GenoGAM (Genome-wide Generalized Additive Model), which brings the well-established and flexible generalized additive models framework to genomic applications using a data parallelism strategy. We model ChIP-Seq read count frequencies as products of smooth functions along chromosomes. Smoothing parameters are objectively estimated from the data by cross-validation, eliminating ad hoc binning and windowing needed by current approaches. GenoGAM provides base-level and region-level significance testing for full factorial designs. Application to a ChIP-Seq dataset in yeast showed increased sensitivity over existing differential occupancy methods while controlling for type I error rate. By analyzing a set of DNA methylation data and illustrating an extension to a peak caller, we further demonstrate the potential of GenoGAM as a generic statistical modeling tool for genome-wide assays.Software is available from Bioconductor: https://www.bioconductor.org/packages/release/bioc/html/GenoGAM.html.Supplementary information is available at Bioinformatics online.},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btx150},
    url = {https://doi.org/10.1093/bioinformatics/btx150},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/33/15/2258/50756539/bioinformatics\_33\_15\_2258.pdf},
}



@article{CHEN2012,
title = {Random forests for genomic data analysis},
journal = {Genomics},
volume = {99},
number = {6},
pages = {323-329},
year = {2012},
issn = {0888-7543},
doi = {https://doi.org/10.1016/j.ygeno.2012.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0888754312000626},
author = {Xi Chen and Hemant Ishwaran},
keywords = {Random forests, Random survival forests, Classification, Prediction, Variable selection, Genomic data analysis},
abstract = {Random forests (RF) is a popular tree-based ensemble machine learning tool that is highly data adaptive, applies to “large p, small n” problems, and is able to account for correlation as well as interactions among features. This makes RF particularly appealing for high-dimensional genomic data analysis. In this article, we systematically review the applications and recent progresses of RF for genomic data, including prediction and classification, variable selection, pathway analysis, genetic association and epistasis detection, and unsupervised learning.}
}


@Article{info15040235,
AUTHOR = {Guido, Rosita and Ferrisi, Stefania and Lofaro, Danilo and Conforti, Domenico},
TITLE = {An Overview on the Advancements of Support Vector Machine Models in Healthcare Applications: A Review},
JOURNAL = {Information},
VOLUME = {15},
YEAR = {2024},
NUMBER = {4},
ARTICLE-NUMBER = {235},
URL = {https://www.mdpi.com/2078-2489/15/4/235},
ISSN = {2078-2489},
ABSTRACT = {Support vector machines (SVMs) are well-known machine learning algorithms for classification and regression applications. In the healthcare domain, they have been used for a variety of tasks including diagnosis, prognosis, and prediction of disease outcomes. This review is an extensive survey on the current state-of-the-art of SVMs developed and applied in the medical field over the years. Many variants of SVM-based approaches have been developed to enhance their generalisation capabilities. We illustrate the most interesting SVM-based models that have been developed and applied in healthcare to improve performance metrics on benchmark datasets, including hybrid classification methods that combine, for instance, optimization algorithms with SVMs. We even report interesting results found in medical applications related to real-world data. Several issues around SVMs, such as selection of hyperparameters and learning from data of questionable quality, are discussed as well. The several variants developed and introduced over the years could be useful in designing new methods to improve performance in critical fields such as healthcare, where accuracy, specificity, and other metrics are crucial. Finally, current research trends and future directions are underlined.},
DOI = {10.3390/info15040235}
}


﻿@Article{vanHilten2024,
author={van Hilten, Arno
and van Rooij, Jeroen
and Heijmans, Bastiaan T.
and 't Hoen, Peter A. C.
and Meurs, Joyce van
and Jansen, Rick
and Franke, Lude
and Boomsma, Dorret I.
and Pool, Ren{\'e}
and van Dongen, Jenny
and Hottenga, Jouke J.
and van Greevenbroek, Marleen M. J.
and Stehouwer, Coen D. A.
and van der Kallen, Carla J. H.
and Schalkwijk, Casper G.
and Wijmenga, Cisca
and Zhernakova, Sasha
and Tigchelaar, Ettje F.
and Slagboom, P. Eline
and Beekman, Marian
and Deelen, Joris
and van Heemst, Diana
and Veldink, Jan H.
and van den Berg, Leonard H.
and van Duijn, Cornelia M.
and Hofman, Bert A.
and Isaacs, Aaron
and Uitterlinden, Andr{\'e} G.
and Jhamai, P. Mila
and Verbiest, Michael
and Suchiman, H. Eka D.
and Verkerk, Marijn
and van der Breggen, Ruud
and Lakenberg, Nico
and Mei, Hailiang
and van Iterson, Maarten
and van Galen, Michiel
and Bot, Jan
and van 't Hof, Peter
and Deelen, Patrick
and Nooren, Irene
and Moed, Matthijs
and Vermaat, Martijn
and Luijk, Ren{\'e}
and Jan Bonder, Marc
and van Dijk, Freerk
and Arindrarto, Wibowo
and Kielbasa, Szymon M.
and Swertz, Morris A.
and van Zwet, Erik. W.
and Ikram, M. Arfan
and Niessen, Wiro J.
and van Meurs, Joyce. B. J.
and Roshchupkin, Gennady V.
and consortium, BIOS},
title={Phenotype prediction using biologically interpretable neural networks on multi-cohort multi-omics data},
journal={npj Systems Biology and Applications},
year={2024},
month={Aug},
day={02},
volume={10},
number={1},
pages={81},
abstract={Integrating multi-omics data into predictive models has the potential to enhance accuracy, which is essential for precision medicine. In this study, we developed interpretable predictive models for multi-omics data by employing neural networks informed by prior biological knowledge, referred to as visible networks. These neural networks offer insights into the decision-making process and can unveil novel perspectives on the underlying biological mechanisms associated with traits and complex diseases. We tested the performance, interpretability and generalizability for inferring smoking status, subject age and LDL levels using genome-wide RNA expression and CpG methylation data from the blood of the BIOS consortium (four population cohorts, Ntotal{\thinspace}={\thinspace}2940). In a cohort-wise cross-validation setting, the consistency of the diagnostic performance and interpretation was assessed. Performance was consistently high for predicting smoking status with an overall mean AUC of 0.95 (95{\%} CI: 0.90--1.00) and interpretation revealed the involvement of well-replicated genes such as AHRR, GPR15 and LRRN3. LDL-level predictions were only generalized in a single cohort with an R2 of 0.07 (95{\%} CI: 0.05--0.08). Age was inferred with a mean error of 5.16 (95{\%} CI: 3.97--6.35) years with the genes COL11A2, AFAP1, OTUD7A, PTPRN2, ADARB2 and CD34 consistently predictive. For both regression tasks, we found that using multi-omics networks improved performance, stability and generalizability compared to interpretable single omic networks. We believe that visible neural networks have great potential for multi-omics analysis; they combine multi-omic data elegantly, are interpretable, and generalize well to data from different cohorts.},
issn={2056-7189},
doi={10.1038/s41540-024-00405-w},
url={https://doi.org/10.1038/s41540-024-00405-w}
}



@Article{s21196595,
AUTHOR = {Geremek, Maciej and Szklanny, Krzysztof},
TITLE = {Deep Learning-Based Analysis of Face Images as a Screening Tool for Genetic Syndromes},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {19},
ARTICLE-NUMBER = {6595},
URL = {https://www.mdpi.com/1424-8220/21/19/6595},
PubMedID = {34640916},
ISSN = {1424-8220},
ABSTRACT = {Approximately 4% of the world’s population suffers from rare diseases. A vast majority of these disorders have a genetic background. The number of genes that have been linked to human diseases is constantly growing, but there are still genetic syndromes that remain to be discovered. The diagnostic yield of genetic testing is continuously developing, and the need for testing is becoming more significant. Due to limited resources, including trained clinical geneticists, patients referred to clinical genetics units must be accurately selected. Around 30–40% of genetic disorders are associated with specific facial characteristics called dysmorphic features. As part of our research, we analyzed the performance of classifiers based on deep learning face recognition models in detecting dysmorphic features. We tested two classification problems: a multiclass problem (15 genetic disorders vs. controls) and a two-class problem (disease vs. controls). In the multiclass task, the best result reached an accuracy level of 84%. The best accuracy result in the two-class problem reached 96%. More importantly, the binary classifier detected disease features in patients with diseases that were not previously present in the training dataset. The classifier was able to generalize differences between patients and controls, and to detect abnormalities without information about the specific disorder. This indicates that a screening tool based on deep learning and facial recognition could not only detect known diseases, but also detect patients with diseases that were not previously known. In the future, this tool could help in screening patients before they are referred to the genetic unit.},
DOI = {10.3390/s21196595}
}



@article{pranav_ia,
author={Pranav,Rajpurkar and Chen,Emma and Oishi,Banerjee and Topol,Eric J.},
year={2022},
month={01},
title={AI in health and medicine},
journal={Nature medicine},
volume={28},
number={1},
pages={31-38},
note={Copyright - © Springer Nature America, Inc. 2022; Última actualización - 2024-10-08; SubjectsTermNotLitGenreText - Social},
abstract={Artificial intelligence (AI) is poised to broadly reshape medicine, potentially improving the experiences of both clinicians and patients. We discuss key findings from a 2-year weekly effort to track and share key developments in medical AI. We cover prospective studies and advances in medical image analysis, which have reduced the gap between research and deployment. We also address several promising avenues for novel medical AI research, including non-image data sources, unconventional problem formulations and human–AI collaboration. Finally, we consider serious technical and ethical challenges in issues spanning from data scarcity to racial bias. As these challenges are addressed, AI’s potential may be realized, making healthcare more accurate, efficient and accessible for patients worldwide.AI has the potential to reshape medicine and make healthcare more accurate, efficient and accessible; this Review discusses recent progress, opportunities and challenges toward achieving this goal.},
keywords={Biology; Medicine; Medical image computing; Artificial intelligence; Health; Racial bias; Accessibility; Image analysis; Medical research; Health care; Medical imaging; Image processing; Patients; Social},
isbn={10788956},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/ai-health-medicine/docview/2623500477/se-2},
}

@ARTICLE{Leung2014-jk,
  title    = "Deep learning of the tissue-regulated splicing code",
  author   = "Leung, Michael K K and Xiong, Hui Yuan and Lee, Leo J and Frey,
              Brendan J",
  abstract = "MOTIVATION: Alternative splicing (AS) is a regulated process that
              directs the generation of different transcripts from single
              genes. A computational model that can accurately predict splicing
              patterns based on genomic features and cellular context is highly
              desirable, both in understanding this widespread phenomenon, and
              in exploring the effects of genetic variations on AS. METHODS:
              Using a deep neural network, we developed a model inferred from
              mouse RNA-Seq data that can predict splicing patterns in
              individual tissues and differences in splicing patterns across
              tissues. Our architecture uses hidden variables that jointly
              represent features in genomic sequences and tissue types when
              making predictions. A graphics processing unit was used to
              greatly reduce the training time of our models with millions of
              parameters. RESULTS: We show that the deep architecture surpasses
              the performance of the previous Bayesian method for predicting AS
              patterns. With the proper optimization procedure and selection of
              hyperparameters, we demonstrate that deep architectures can be
              beneficial, even with a moderately sparse dataset. An analysis of
              what the model has learned in terms of the genomic features is
              presented.",
  journal  = "Bioinformatics",
  volume   =  30,
  number   =  12,
  pages    = "i121--9",
  month    =  jun,
  year     =  2014,
  address  = "England",
  language = "en"
}
@article{economic_stjhon,
author={St John,Andrew and Price,Christopher P.},

year={2013},

title={Economic Evidence and Point-of-Care Testing},

journal={Clinical biochemist reviews},

volume={34},

number={2},

pages={61-74},

abstract={Health economics has been an established feature of the research, policymaking, practice and management in the delivery of healthcare. However its role is increasing as the cost of healthcare begins to drive changes in most healthcare systems. Thus the output from cost effectiveness studies is now being taken into account when making reimbursement decisions, e.g. in Australia and the United Kingdom. Against this background it is also recognised that the health economic tools employed in healthcare, and particularly the output from the use of these tools however, are not always employed in the routine delivery of services. One of the notable consequences of this situation is the poor record of innovation in healthcare with respect to the adoption of new technologies, and the realisation of their benefits. The evidence base for the effectiveness of diagnostic services is well known to be limited, and one consequence of this has been a very limited literature on cost effectiveness. One reason for this situation is undoubtedly the reimbursement strategies employed in laboratory medicine for many years, simplistically based on the complexity of the test procedure, and the delivery as a cost-per-test service. This has proved a disincentive to generate the required evidence, and little effort to generate an integrated investment and disinvestment business case, associated with care pathway changes. Point-of-care testing creates a particularly challenging scenario because, on the one hand, the unit cost-per-test is larger through the loss of the economy of scale offered by automation, whilst it offers the potential of substantial savings through enabling rapid delivery of results, and reduction of facility costs. This is important when many health systems are planning for complete system redesign. We review the literature on economic assessment of point-of-care testing in the context of these developments.;Health economics has been an established feature of the research, policymaking, practice and management in the delivery of healthcare. However its role is increasing as the cost of healthcare begins to drive changes in most healthcare systems. Thus the output from cost effectiveness studies is now being taken into account when making reimbursement decisions, e.g. in Australia and the United Kingdom. Against this background it is also recognised that the health economic tools employed in healthcare, and particularly the output from the use of these tools however, are not always employed in the routine delivery of services. One of the notable consequences of this situation is the poor record of innovation in healthcare with respect to the adoption of new technologies, and the realisation of their benefits.

The evidence base for the effectiveness of diagnostic services is well known to be limited, and one consequence of this has been a very limited literature on cost effectiveness. One reason for this situation is undoubtedly the reimbursement strategies employed in laboratory medicine for many years, simplistically based on the complexity of the test procedure, and the delivery as a cost-per-test service. This has proved a disincentive to generate the required evidence, and little effort to generate an integrated investment and disinvestment business case, associated with care pathway changes.

Point-of-care testing creates a particularly challenging scenario because, on the one hand, the unit cost-per-test is larger through the loss of the economy of scale offered by automation, whilst it offers the potential of substantial savings through enabling rapid delivery of results, and reduction of facility costs. This is important when many health systems are planning for complete system redesign. We review the literature on economic assessment of point-of-care testing in the context of these developments.;Health economics has been an established feature of the research, policymaking, practice and management in the delivery of healthcare. However its role is increasing as the cost of healthcare begins to drive changes in most healthcare systems. Thus the output from cost effectiveness studies is now being taken into account when making reimbursement decisions, e.g. in Australia and the United Kingdom. Against this background it is also recognised that the health economic tools employed in healthcare, and particularly the output from the use of these tools however, are not always employed in the routine delivery of services. One of the notable consequences of this situation is the poor record of innovation in healthcare with respect to the adoption of new technologies, and the realisation of their benefits. The evidence base for the effectiveness of diagnostic services is well known to be limited, and one consequence of this has been a very limited literature on cost effectiveness. One reason for this situation is undoubtedly the reimbursement strategies employed in laboratory medicine for many years, simplistically based on the complexity of the test procedure, and the delivery as a cost-per-test service. This has proved a disincentive to generate the required evidence, and little effort to generate an integrated investment and disinvestment business case, associated with care pathway changes. Point-of-care testing creates a particularly challenging scenario because, on the one hand, the unit cost-per-test is larger through the loss of the economy of scale offered by automation, whilst it offers the potential of substantial savings through enabling rapid delivery of results, and reduction of facility costs. This is important when many health systems are planning for complete system redesign. We review the literature on economic assessment of point-of-care testing in the context of these developments.Health economics has been an established feature of the research, policymaking, practice and management in the delivery of healthcare. However its role is increasing as the cost of healthcare begins to drive changes in most healthcare systems. Thus the output from cost effectiveness studies is now being taken into account when making reimbursement decisions, e.g. in Australia and the United Kingdom. Against this background it is also recognised that the health economic tools employed in healthcare, and particularly the output from the use of these tools however, are not always employed in the routine delivery of services. One of the notable consequences of this situation is the poor record of innovation in healthcare with respect to the adoption of new technologies, and the realisation of their benefits. The evidence base for the effectiveness of diagnostic services is well known to be limited, and one consequence of this has been a very limited literature on cost effectiveness. One reason for this situation is undoubtedly the reimbursement strategies employed in laboratory medicine for many years, simplistically based on the complexity of the test procedure, and the delivery as a cost-per-test service. This has proved a disincentive to generate the required evidence, and little effort to generate an integrated investment and disinvestment business case, associated with care pathway changes. Point-of-care testing creates a particularly challenging scenario because, on the one hand, the unit cost-per-test is larger through the loss of the economy of scale offered by automation, whilst it offers the potential of substantial savings through enabling rapid delivery of results, and reduction of facility costs. This is important when many health systems are planning for complete system redesign. We review the literature on economic assessment of point-of-care testing in the context of these developments.;},

keywords={Review},

isbn={0159-8090},

language={English},

}


@article{deep_angemuller_2016,
author={Angermueller,Christof and Tanel Pärnamaa and Parts,Leopold and Stegle,Oliver},
year={2016},
month={07},
title={Deep learning for computational biology},
journal={Molecular Systems Biology},
volume={12},
number={7},
note={Copyright - © 2016. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License; Última actualización - 2024-10-06; SubjectsTermNotLitGenreText - Environmental},
abstract={Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.},
keywords={Biology; cellular imaging; computational biology; deep learning; machine learning; regulatory genomics; Software; Neurons; Gene expression; Principal components analysis; Artificial intelligence; Councils; Neural networks; Computer applications; Genomics; Learning algorithms; New technology; Environmental},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/deep-learning-computational-biology/docview/2290547777/se-2},
}

@article{1364-503X,

author={Johnstone,Iain M. and Titterington,D. M.},

year={2009},

title={Statistical challenges of high-dimensional data},

journal={Philosophical transactions of the Royal Society of London. Series A: Mathematical, physical, and engineering sciences},

volume={367},

number={1906},

pages={4237-4253},

abstract={Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements

on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response:

the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the

difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a

taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements.

We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The

topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables

that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle

computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in

the issue.;Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response: the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements. We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in the issue.;Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response: the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements. We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in the issue.Modern applications of statistical theory and methods can involve extremely large datasets, often with huge numbers of measurements on each of a comparatively small number of experimental units. New methodology and accompanying theory have emerged in response: the goal of this Theme Issue is to illustrate a number of these recent developments. This overview article introduces the difficulties that arise with high-dimensional data in the context of the very familiar linear statistical model: we give a taste of what can nevertheless be achieved when the parameter vector of interest is sparse, that is, contains many zero elements. We describe other ways of identifying low-dimensional subspaces of the data space that contain all useful information. The topic of classification is then reviewed along with the problem of identifying, from within a very large set, the variables that help to classify observations. Brief mention is made of the visualization of high-dimensional data and ways to handle computational problems in Bayesian analysis are described. At appropriate points, reference is made to the other papers in the issue.;},

keywords={Bayes Theorem; Bayesian Analysis; Classification; Cluster Analysis; High-Dimensional Data; Linear Models; Regression; Sparsity; Statistics as Topic - methods},

isbn={1364-503X},

language={English},

}

@article{rudin_2019 ,
author={Rudin, Cynthia},
year={2019},
month={05},
title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
journal={Nature Machine Intelligence},
volume={1},
number={5},
pages={206-215},
note={Copyright - © Springer Nature Limited 2019; Última actualización - 2023-11-30},
abstract={Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.There has been a recent rise of interest in developing methods for ‘explainable AI’, where models are created to explain how a first ‘black box’ machine learning model arrives at a specific decision. It can be argued that instead efforts should be directed at building inherently interpretable models in the first place, in particular where they are applied in applications that directly affect human lives, such as in healthcare and criminal justice.},
keywords={Computers--Artificial Intelligence; Machine learning; Health care; Criminal justice; Explainable artificial intelligence; Sparsity; Accuracy; Deep learning; Datasets; Crime; Artificial intelligence; Knowledge discovery; Decision making; Mathematical models; Data science; Boxes; Computer vision; Algorithms},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/stop-explaining-black-box-machine-learning-models/docview/2622642402/se-2},
}







@article{rajkomar_2019,
author={Alvin,Rajkomar and Dean,Jeffrey and Isaac,Kohane},
year={2019},
month={Apr 04},
title={Machine Learning in Medicine},
journal={The New England journal of medicine},
volume={380},
number={14},
pages={1347-1358},
note={Copyright - Copyright © 2019 Massachusetts Medical Society. All rights reserved; Última actualización - 2024-03-22},
abstract={In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. These data are collected and curated to provide the latest evidence-based assessment and recommendations.},
keywords={Medical Sciences; Machine learning; Health informatics; Data mining; Medicine; Patients; Data processing; Algorithms; Melanoma; Health care; Decision making; Learning algorithms; Biopsy; Clinical decision making; Artificial intelligence},
isbn={00284793},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/machine-learning-medicine/docview/2213944150/se-2},
}



@article{Ginsburg_2018,
author={Ginsburg,Geoffrey S. and Phillips,Kathryn A.},
year={2018},
month={05},
title={Precision Medicine: From Science To Value},
journal={Health affairs},
volume={37},
number={5},
pages={694-701},
note={Copyright - Copyright The People to People Health Foundation, Inc., Project HOPE May 2018; Última actualización - 2025-01-27; SubjectsTermNotLitGenreText - United States--US; Social},
abstract={Precision medicine is making an impact on patients, health care delivery systems, and research participants in ways that were only imagined fifteen years ago when the human genome was first sequenced. Discovery of disease-causing and drug-response genetic variants has accelerated, while adoption into clinical medicine has lagged. We define precision medicine and the stakeholder community required to enable its integration into research and health care. We explore the intersection of data science, analytics, and precision medicine in the formation of health systems that carry out research in the context of clinical care and that optimize the tools and information used to deliver improved patient outcomes. We provide examples of real-world impact and conclude with a policy and economic agenda necessary for the adoption of this new paradigm of health care both in the United States and globally.},
keywords={Public Health And Safety; Precision medicine; Analytics; Data science; Cancer; Genetic variance; Clinical medicine; Genomes; Health care; Health care delivery; Genomics; Patients; Medicine; Impact analysis; Research; Information technology; Health care policy; Market research; Information sharing; FDA approval; Medical records; Disease; Blood clots; Medical laboratories; Fetuses; Electronic health records; Decision making; Consortia; Clinical outcomes; Variants; Clinical decision making; Genetic diversity; Integrated care; Underserved populations; Social; United States--US; 62151:Medical and Diagnostic Laboratories},
isbn={02782715},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/precision-medicine-science-value/docview/2039208059/se-2},
}


@article{Salehi_2019,

author={Salehi,Fariborz and Abbasi,Ehsan and Hassibi,Babak},

year={2019},

title={The Impact of Regularization on High-dimensional Logistic Regression},

abstract={Proceedings of NeurIPS 2019 Logistic regression is commonly used for modeling dichotomous outcomes. In  the classical setting, where the number of observations is much larger than the  number of parameters, properties of the maximum likelihood estimator in  logistic regression are well understood. Recently, Sur and Candes have studied  logistic regression in the high-dimensional regime, where the number of  observations and parameters are comparable, and show, among other things, that  the maximum likelihood estimator is biased. In the high-dimensional regime the  underlying parameter vector is often structured (sparse, block-sparse,  finite-alphabet, etc.) and so in this paper we study regularized logistic  regression (RLR), where a convex regularizer that encourages the desired  structure is added to the negative of the log-likelihood function. An advantage  of RLR is that it allows parameter recovery even for instances where the  (unconstrained) maximum likelihood estimate does not exist. We provide a  precise analysis of the performance of RLR via the solution of a system of six  nonlinear equations, through which any performance metric of interest (mean,  mean-squared error, probability of support recovery, etc.) can be explicitly  computed. Our results generalize those of Sur and Candes and we provide a  detailed study for the cases of $\ell_2^2$-RLR and sparse  ($\ell_1$-regularized) logistic regression. In both cases, we obtain explicit  expressions for various performance metrics and can find the values of the  regularizer parameter that optimizes the desired performance. The theory is  validated by extensive numerical simulations across a range of parameter values  and problem instances.},

keywords={Computer Science - Information Theory; Computer Science - Learning; Mathematics - Information Theory; Mathematics - Probability; Statistics - Machine Learning},

language={English},

}


@article{chafai_2023,

author={Chafai,Narjice and Hayah,Ichrak and Houaga,Isidore and Badaoui,Bouabid},

year={2023},

title={A review of machine learning models applied to genomic prediction in animal breeding},

journal={Frontiers in genetics},

volume={14},

pages={1150596},

abstract={The advent of modern genotyping technologies has revolutionized genomic selection in animal breeding. Large marker datasets have shown several drawbacks for traditional genomic prediction methods in terms of flexibility, accuracy, and computational power. Recently, the application of machine learning models in animal breeding has gained a lot of interest due to their tremendous flexibility and their ability to capture patterns in large noisy datasets. Here, we present a general overview of a handful of machine learning algorithms and their application in genomic prediction to provide a meta-picture of their performance in genomic estimated breeding values estimation, genotype imputation, and feature selection. Finally, we discuss a potential adoption of machine learning models in genomic prediction in developing countries. The results of the reviewed studies showed that machine learning models have indeed performed well in fitting large noisy data sets and modeling minor nonadditive effects in some of the studies. However, sometimes conventional methods outperformed machine learning models, which confirms that there’s no universal method for genomic prediction. In summary, machine learning models have great potential for extracting patterns from single nucleotide polymorphism datasets. Nonetheless, the level of their adoption in animal breeding is still low due to data limitations, complex genetic interactions, a lack of standardization and reproducibility, and the lack of interpretability of machine learning models when trained with biological data. Consequently, there is no remarkable outperformance of machine learning methods compared to traditional methods in genomic prediction. Therefore, more research should be conducted to discover new insights that could enhance livestock breeding programs.;The advent of modern genotyping technologies has revolutionized genomic selection in animal breeding. Large marker datasets have shown several drawbacks for traditional genomic prediction methods in terms of flexibility, accuracy, and computational power. Recently, the application of machine learning models in animal breeding has gained a lot of interest due to their tremendous flexibility and their ability to capture patterns in large noisy datasets. Here, we present a general overview of a handful of machine learning algorithms and their application in genomic prediction to provide a meta-picture of their performance in genomic estimated breeding values estimation, genotype imputation, and feature selection. Finally, we discuss a potential adoption of machine learning models in genomic prediction in developing countries. The results of the reviewed studies showed that machine learning models have indeed performed well in fitting large noisy data sets and modeling minor nonadditive effects in some of the studies. However, sometimes conventional methods outperformed machine learning models, which confirms that there's no universal method for genomic prediction. In summary, machine learning models have great potential for extracting patterns from single nucleotide polymorphism datasets. Nonetheless, the level of their adoption in animal breeding is still low due to data limitations, complex genetic interactions, a lack of standardization and reproducibility, and the lack of interpretability of machine learning models when trained with biological data. Consequently, there is no remarkable outperformance of machine learning methods compared to traditional methods in genomic prediction. Therefore, more research should be conducted to discover new insights that could enhance livestock breeding programs.The advent of modern genotyping technologies has revolutionized genomic selection in animal breeding. Large marker datasets have shown several drawbacks for traditional genomic prediction methods in terms of flexibility, accuracy, and computational power. Recently, the application of machine learning models in animal breeding has gained a lot of interest due to their tremendous flexibility and their ability to capture patterns in large noisy datasets. Here, we present a general overview of a handful of machine learning algorithms and their application in genomic prediction to provide a meta-picture of their performance in genomic estimated breeding values estimation, genotype imputation, and feature selection. Finally, we discuss a potential adoption of machine learning models in genomic prediction in developing countries. The results of the reviewed studies showed that machine learning models have indeed performed well in fitting large noisy data sets and modeling minor nonadditive effects in some of the studies. However, sometimes conventional methods outperformed machine learning models, which confirms that there's no universal method for genomic prediction. In summary, machine learning models have great potential for extracting patterns from single nucleotide polymorphism datasets. Nonetheless, the level of their adoption in animal breeding is still low due to data limitations, complex genetic interactions, a lack of standardization and reproducibility, and the lack of interpretability of machine learning models when trained with biological data. Consequently, there is no remarkable outperformance of machine learning methods compared to traditional methods in genomic prediction. Therefore, more research should be conducted to discover new insights that could enhance livestock breeding programs.;},

keywords={algorithms; animal breeding; artificial intelligence; classification; Genetics; genomic selection; regression},

isbn={1664-8021},

language={English},

}

@article{kocejko_2024,
author={Kocejko,Tomasz},
year={2024},
title={A Data-Driven Comparative Analysis of Machine-Learning Models for Familial Hypercholesterolemia Detection},
journal={Applied Sciences},
volume={14},
number={23},
pages={11187},
note={Copyright - © 2024 by the author. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License; Última actualización - 2024-12-13; SubjectsTermNotLitGenreText - Poland},
abstract={Featured ApplicationThe presented study can contribute to increasing the familial hypercholesterolemia classification and may help reduce the number of undiagnosed cases of the disease.AbstractThis study presents an assessment of familial hypercholesterolemia (FH) probability using different algorithms (CatBoost, XGBoost, Random Forest, SVM) and its ensembles, leveraging electronic health record data. The primary objective is to explore an enhanced method for estimating FH probability, surpassing the currently recommended Dutch Lipid Clinic Network (DLCN) Score. The models were trained using the largest Polish cohort of patients enrolled in an FH clinic, all of whom underwent genetic testing for FH-associated mutations. The initial dataset comprised over 100 parameters per patient, which was reduced to 48 clinically accessible features to ensure applicability in routine outpatient settings. To preserve balance, the data were stratified according to DLCN score ranges (, , , and ≥9), representing varying levels of FH likelihood. The dataset was then split into training and test sets with an 80/20 ratio. Machine-learning models were trained, with hyperparameters optimized via grid search. The accuracy of the DLCN score in predicting FH was first evaluated by examining the proportion of patients with positive DNA tests relative to those with a DLCN score of 6 and above, the threshold for genetic testing. The DLCN score demonstrated an accuracy of approximately 40%. In contrast, the CatBoost model and its ensembles achieved over 80% accuracy. While the DLCN score remains a clinically valuable tool, its diagnostic accuracy is limited. The findings indicate that the ML models offer a substantial improvement in the precision of FH diagnosis, demonstrating its potential to enhance clinical decision making in identifying patients with FH.},
keywords={Sciences: Comprehensive Works; machine learning; familial hypercholesterolemia; DLCN; model ensembles; Genetic testing; Patients; Accuracy; Electronic health records; Datasets; Artificial intelligence; Boolean; Neural networks; Classification; Cholesterol; Support vector machines; Cardiovascular disease; Lipoproteins; Algorithms; Lipids; Decision trees; Kinases; Heart rate; Poland},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/data-driven-comparative-analysis-machine-learning/docview/3143935683/se-2},
}


@article{LeCun_2015,
author={LeCun,Yann and Bengio,Yoshua and Hinton,Geoffrey},
year={2015},
month={May 28},
title={Deep learning},
journal={Nature, suppl.INSIGHT: MACHINE INTELLIGENCE},
volume={521},
number={7553},
pages={436-444},
note={Copyright - Copyright Nature Publishing Group May 28, 2015; Características del documento - ; Diagrams; Photographs; Graphs; Última actualización - 2024-10-04; CODEN - NATUAS},
abstract={Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
keywords={Environmental Studies; Backpropagation; Deep learning; Speech recognition; Classification; Algorithms; Neural networks; Natural language; Artificial intelligence},
isbn={00280836},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/deep-learning/docview/1685003444/se-2},
}


@article{Hsieh_2019,

author={Hsieh,Tzung-Chien and Mensah,Martin A. and Pantel,Jean T. and Aguilar,Dione and Bar,Omri and Bayat,Allan and Becerra-Solano,Luis and Bentzen,Heidi B. and Biskup,Saskia and Borisov,Oleg and Braaten,Oivind and Ciaccio,Claudia and Coutelier,Marie and Cremer,Kirsten and Danyel,Magdalena and Daschkey,Svenja and Eden,Hilda D. and Devriendt,Koenraad and Wilson,Sandra and Douzgou,Sofia and Đukić,Dejan and Ehmke,Nadja and Fauth,Christine and Fischer-Zirnsak,Björn and Fleischer,Nicole and Gabriel,Heinz and Graul-Neumann,Luitgard and Gripp,Karen W. and Gurovich,Yaron and Gusina,Asya and Haddad,Nechama and Hajjir,Nurulhuda and Hanani,Yair and Hertzberg,Jakob and Hoertnagel,Konstanze and Howell,Janelle and Ivanovski,Ivan and Kaindl,Angela and Kamphans,Tom and Kamphausen,Susanne and Karimov,Catherine and Kathom,Hadil and Keryan,Anna and Knaus,Alexej and Köhler,Sebastian and Kornak,Uwe and Lavrov,Alexander and Leitheiser,Maximilian and Lyon,Gholson J. and Mangold,Elisabeth and Reina,Purificación M. and Carrascal,Antonio M. and Mitter,Diana and Herrador,Laura M. and Nadav,Guy and Nöthen,Markus and Orrico,Alfredo and Ott,Claus-Eric and Park,Kristen and Peterlin,Borut and Pölsler,Laura and Raas-Rothschild,Annick and Randolph,Linda and Revencu,Nicole and Fagerberg,Christina R. and Robinson,Peter N. and Rosnev,Stanislav and Rudnik,Sabine and Rudolf,Gorazd and Schatz,Ulrich and Schossig,Anna and Schubach,Max and Shanoon,Or and Sheridan,Eamonn and Smirin-Yosef,Pola and Spielmann,Malte and Suk,Eun-Kyung and Sznajer,Yves and Thiel,Christian T. and Thiel,Gundula and Verloes,Alain and Vrecar,Irena and Wahl,Dagmar and Weber,Ingrid and Winter,Korina and Wiśniewska,Marzena and Wollnik,Bernd and Yeung,Ming W. and Zhao,Max and Zhu,Na and Zschocke,Johannes and Mundlos,Stefan and Horn,Denise and Krawitz,Peter M.},

year={2019},

title={PEDIA: prioritization of exome data by image analysis},

journal={Genetics in medicine},

volume={21},

number={12},

pages={2807-2814},

abstract={Purpose

Phenotype information is crucial for the interpretation of genomic variants. So far it has only been accessible for bioinformatics workflows after encoding into clinical terms by expert dysmorphologists.

Methods

Here, we introduce an approach driven by artificial intelligence that uses portrait photographs for the interpretation of clinical exome data. We measured the value added by computer-assisted image analysis to the diagnostic yield on a cohort consisting of 679 individuals with 105 different monogenic disorders. For each case in the cohort we compiled frontal photos, clinical features, and the disease-causing variants, and simulated multiple exomes of different ethnic backgrounds.

Results

The additional use of similarity scores from computer-assisted analysis of frontal photos improved the top 1 accuracy rate by more than 20–89% and the top 10 accuracy rate by more than 5–99% for the disease-causing gene.

Conclusion

Image analysis by deep-learning algorithms can be used to quantify the phenotypic similarity (PP4 criterion of the American College of Medical Genetics and Genomics guidelines) and to advance the performance of bioinformatics pipelines for exome analysis.;Phenotype information is crucial for the interpretation of genomic variants. So far it has only been accessible for bioinformatics workflows after encoding into clinical terms by expert dysmorphologists.

Here, we introduce an approach driven by artificial intelligence that uses portrait photographs for the interpretation of clinical exome data. We measured the value added by computer-assisted image analysis to the diagnostic yield on a cohort consisting of 679 individuals with 105 different monogenic disorders. For each case in the cohort we compiled frontal photos, clinical features, and the disease-causing variants, and simulated multiple exomes of different ethnic backgrounds.

The additional use of similarity scores from computer-assisted analysis of frontal photos improved the top 1 accuracy rate by more than 20-89% and the top 10 accuracy rate by more than 5-99% for the disease-causing gene.

Image analysis by deep-learning algorithms can be used to quantify the phenotypic similarity (PP4 criterion of the American College of Medical Genetics and Genomics guidelines) and to advance the performance of bioinformatics pipelines for exome analysis.;PurposePhenotype information is crucial for the interpretation of genomic variants. So far it has only been accessible for bioinformatics workflows after encoding into clinical terms by expert dysmorphologists.MethodsHere, we introduce an approach driven by artificial intelligence that uses portrait photographs for the interpretation of clinical exome data. We measured the value added by computer-assisted image analysis to the diagnostic yield on a cohort consisting of 679 individuals with 105 different monogenic disorders. For each case in the cohort we compiled frontal photos, clinical features, and the disease-causing variants, and simulated multiple exomes of different ethnic backgrounds.ResultsThe additional use of similarity scores from computer-assisted analysis of frontal photos improved the top 1 accuracy rate by more than 20–89% and the top 10 accuracy rate by more than 5–99% for the disease-causing gene.ConclusionImage analysis by deep-learning algorithms can be used to quantify the phenotypic similarity (PP4 criterion of the American College of Medical Genetics and Genomics guidelines) and to advance the performance of bioinformatics pipelines for exome analysis.;Phenotype information is crucial for the interpretation of genomic variants. So far it has only been accessible for bioinformatics workflows after encoding into clinical terms by expert dysmorphologists.PURPOSEPhenotype information is crucial for the interpretation of genomic variants. So far it has only been accessible for bioinformatics workflows after encoding into clinical terms by expert dysmorphologists.Here, we introduce an approach driven by artificial intelligence that uses portrait photographs for the interpretation of clinical exome data. We measured the value added by computer-assisted image analysis to the diagnostic yield on a cohort consisting of 679 individuals with 105 different monogenic disorders. For each case in the cohort we compiled frontal photos, clinical features, and the disease-causing variants, and simulated multiple exomes of different ethnic backgrounds.METHODSHere, we introduce an approach driven by artificial intelligence that uses portrait photographs for the interpretation of clinical exome data. We measured the value added by computer-assisted image analysis to the diagnostic yield on a cohort consisting of 679 individuals with 105 different monogenic disorders. For each case in the cohort we compiled frontal photos, clinical features, and the disease-causing variants, and simulated multiple exomes of different ethnic backgrounds.The additional use of similarity scores from computer-assisted analysis of frontal photos improved the top 1 accuracy rate by more than 20-89% and the top 10 accuracy rate by more than 5-99% for the disease-causing gene.RESULTSThe additional use of similarity scores from computer-assisted analysis of frontal photos improved the top 1 accuracy rate by more than 20-89% and the top 10 accuracy rate by more than 5-99% for the disease-causing gene.Image analysis by deep-learning algorithms can be used to quantify the phenotypic similarity (PP4 criterion of the American College of Medical Genetics and Genomics guidelines) and to advance the performance of bioinformatics pipelines for exome analysis.CONCLUSIONImage analysis by deep-learning algorithms can be used to quantify the phenotypic similarity (PP4 criterion of the American College of Medical Genetics and Genomics guidelines) and to advance the performance of bioinformatics pipelines for exome analysis.;},

keywords={Algorithms; Bioinformatics; Biomedical and Life Sciences; Biomedicine; Computational Biology - methods; Databases, Genetic; Deep Learning; Exome - genetics; Female; Genomics; Human Genetics; Humans; Image Processing, Computer-Assisted - methods; Laboratory Medicine; Male; Phenotype; Sequence Analysis, DNA - methods; Software},

isbn={1098-3600;1530-0366;},

language={English},

}


@article{bush_2012,
author={Bush,William S. and Moore,Jason H.},
year={2012},
month={12},
title={Chapter 11: Genome-Wide Association Studies},
journal={PLoS Computational Biology},
volume={8},
number={12},
pages={e1002822},
note={Copyright - © 2012 Bush, Moore. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited: Bush WS, Moore JH (2012) Chapter 11: Genome-Wide Association Studies. PLoS Comput Biol 8(12): e1002822. doi:10.1371/journal.pcbi.1002822; Última actualización - 2024-11-07; SubjectsTermNotLitGenreText - United States--US; Social},
abstract={Genome-wide association studies (GWAS) have evolved over the last ten years into a powerful tool for investigating the genetic architecture of human disease. In this work, we review the key concepts underlying GWAS, including the architecture of common diseases, the structure of common human genetic variation, technologies for capturing genetic information, study designs, and the statistical methods used for data analysis. We also look forward to the future beyond GWAS.},
keywords={Biology--Computer Applications; Genetic variation; Statistics; Genome; Association studies; Genome-wide association study; Data analysis; Association; Genetics; Genes; Genomics; Deoxyribonucleic acid--DNA; Statistical methods; Disease; Blood clots; Macular degeneration; Population; Risk factors; Hypotheses; Drug dosages; Bioinformatics; Genomes; Hypothesis testing; Genetic testing; Diabetes; Health risk assessment; Bankruptcy; Genetic diversity; Social; United States--US; Haplotypes; Phenotype; Humans; Electronic Health Records; Genetic Linkage},
isbn={1553734X},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/scholarly-journals/chapter-11-genome-wide-association-studies/docview/1313184347/se-2},
}


@book{hosmer_2013,

author={Hosmer,David W. and Lemeshow,Stanley and Sturdivant,Rodney X.},

year={2013},

title={Applied Logistic Regression},

publisher={Wiley},

address={Hoboken, N.J},

edition={3. Aufl.;3;3rd;3rd;},

abstract={ A new edition of the definitive guide to logistic regression modeling for health science and other applications This thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables. Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include: A chapter on the analysis of correlated outcome data A wealth of additional material for topics ranging from Bayesian methods to assessing model fit Rich data sets from real-world studies that demonstrate each method under discussion Detailed examples and interpretation of the presented results as well as exercises throughout Applied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines.;A new edition of the definitive guide to logistic regression modeling for health science and other applicationsThis thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables.Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include:A chapter on the analysis of correlated outcome dataA wealth of additional material for topics ranging from Bayesian methods to assessing model fitRich data sets from real-world studies that demonstrate each method under discussionDetailed examples and interpretation of the presented results as well as exercises throughoutApplied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines.;A new edition of the definitive guide to logistic regression modeling for health science and other applications This thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables. Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include: * A chapter on the analysis of correlated outcome data * A wealth of additional material for topics ranging from Bayesian methods to assessing model fit * Rich data sets from real-world studies that demonstrate each method under discussion * Detailed examples and interpretation of the presented results as well as exercises throughout Applied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines.;},

keywords={Regression analysis},

isbn={0470582472;9780470582473;9781118548356;1118548353;9781118548387;1118548388;},

language={English},

}

@article{wutong_2009,

author={Wu,Tong T. and Chen,Yi F. and Hastie,Trevor and Sobel,Eric and Lange,Kenneth},

year={2009},

title={Genome-wide association analysis by lasso penalized logistic regression},

journal={Bioinformatics},

volume={25},

number={6},

pages={714-721},

abstract={Motivation:

In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.

Method:

The present article evaluates the performance of lasso penalized logistic regression in case–control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.

Results:

This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.

Availability:

The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.

Contact:

klange@ucla.edu

Supplementary information:

Supplementary data

are available at

Bioinformatics

online.;In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.

The present article evaluates the performance of lasso penalized logistic regression in case-control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.

This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.

The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.

Supplementary data are available at Bioinformatics online.;In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.MOTIVATIONIn ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.The present article evaluates the performance of lasso penalized logistic regression in case-control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.METHODThe present article evaluates the performance of lasso penalized logistic regression in case-control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.RESULTSThis strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.AVAILABILITYThe software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.Supplementary data are available at Bioinformatics online.SUPPLEMENTARY INFORMATIONSupplementary data are available at Bioinformatics online.;Motivation: In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.Method: The present article evaluates the performance of lasso penalized logistic regression in case-control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.Results: This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.Availability: The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.Contact: klange super(c)la.eduSupplementary information: Supplementary data are available at Bioinformatics online.;Motivation: In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.

Method: The present article evaluates the performance of lasso penalized logistic regression in case-control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.

Results: This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.

Availability: The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.

Contact:

klange@ucla.edu

Supplementary information:

Supplementary data are available at Bioinformatics online.;Motivation: In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations. Method: The present article evaluates the performance of lasso penalized logistic regression in case–control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression. Results: This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs. Availability: The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site. Contact: klange@ucla.edu Supplementary information: Supplementary data are available at Bioinformatics online.;Motivation: In ordinary regression, imposition of a lasso penalty makes continuous model selection straightforward. Lasso penalized regression is particularly advantageous when the number of predictors far exceeds the number of observations.

Method: The present article evaluates the performance of lasso penalized logistic regression in case–control disease gene mapping with a large number of SNPs (single nucleotide polymorphisms) predictors. The strength of the lasso penalty can be tuned to select a predetermined number of the most relevant SNPs and other predictors. For a given value of the tuning constant, the penalized likelihood is quickly maximized by cyclic coordinate ascent. Once the most potent marginal predictors are identified, their two-way and higher order interactions can also be examined by lasso penalized logistic regression.

Results: This strategy is tested on both simulated and real data. Our findings on coeliac disease replicate the previous SNP results and shed light on possible interactions among the SNPs.

Availability: The software discussed is available in Mendel 9.0 at the UCLA Human Genetics web site.

Contact:  klange@ucla.edu

Supplementary information:  Supplementary data are available at Bioinformatics online.;},

keywords={Bioinformatics; Biological and medical sciences; Computational Biology - methods; Fundamental and applied biological sciences. Psychology; Gene mapping; General aspects; Genetics; Genome-Wide Association Study; Internet; Logistic Models; Mathematics in biology. Statistical analysis. Models. Metrology. Data processing in biology (general aspects); Original Papers; Polymorphism, Single Nucleotide; Software},

isbn={1367-4803;1367-4811;},

language={English},

}



@article{10.1093/eurheartj/ehu207,
    author = {Steyerberg, Ewout W. and Vergouwe, Yvonne},
    title = {Towards better clinical prediction models: seven steps for development and an ABCD for validation},
    journal = {European Heart Journal},
    volume = {35},
    number = {29},
    pages = {1925-1931},
    year = {2014},
    month = {06},
    abstract = {Clinical prediction models provide risk estimates for the presence of disease (diagnosis) or an event in the future course of disease (prognosis) for individual patients. Although publications that present and evaluate such models are becoming more frequent, the methodology is often suboptimal. We propose that seven steps should be considered in developing prediction models: (i) consideration of the research question and initial data inspection; (ii) coding of predictors; (iii) model specification; (iv) model estimation; (v) evaluation of model performance; (vi) internal validation; and (vii) model presentation. The validity of a prediction model is ideally assessed in fully independent data, where we propose four key measures to evaluate model performance: calibration-in-the-large, or the model intercept (A); calibration slope (B); discrimination, with a concordance statistic (C); and clinical usefulness, with decision-curve analysis (D). As an application, we develop and validate prediction models for 30-day mortality in patients with an acute myocardial infarction. This illustrates the usefulness of the proposed framework to strengthen the methodological rigour and quality for prediction models in cardiovascular research.},
    issn = {0195-668X},
    doi = {10.1093/eurheartj/ehu207},
    url = {https://doi.org/10.1093/eurheartj/ehu207},
    eprint = {https://academic.oup.com/eurheartj/article-pdf/35/29/1925/6731119/ehu207.pdf},
}


@article{Libbrecht2015,

author={Libbrecht,Maxwell W. and Noble,William S.},

year={2015},

title={Machine learning applications in genetics and genomics},

journal={Nature reviews. Genetics},

volume={16},

number={6},

pages={321-332},

abstract={The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.;The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.;Key Points

The field of machine learning includes the development and application of computer algorithms that improve with experience.

Machine learning methods can be divided into supervised, semi-supervised and unsupervised methods. Supervised methods are trained on examples with labels (for example, 'gene' or 'not gene') and are then used to predict these labels on other examples, whereas unsupervised methods find patterns in data sets without the use of labels. Semi-supervised methods combine these two approaches, leveraging patterns in unlabelled data to improve power in the prediction of labels.

Different machine learning methods may be required for an application, depending on whether one is interested in interpreting the output model or is simply concerned with predictive power. Generative models, which posit a probabilistic distribution over input data, are generally best for interpretability, whereas discriminative models, which seek only to model labels, are generally best for predictive power.

Prior information can be added to a model in order to train the model more effectively when it is provided with limited data, to limit the complexity of the model or to incorporate data that are not used by the model directly. Prior information can be incorporated explicitly in a probabilistic model or implicitly through the choice of features or similarity measures.

The choice of an appropriate performance measure depends strongly on the application task. Machine learning methods are most effective when they optimize an appropriate performance measure.

Network estimation methods are appropriate when the data contain complex dependencies among examples. These methods work best when they take into account the confounding effects of indirect relationships.

Machine learning methods are becoming increasingly important in the analysis of large-scale genomic, epigenomic, proteomic and metabolic data sets. In this Review, the authors consider the applications of supervised, semi-supervised and unsupervised machine learning methods to genetic and genomic studies. They provide general guidelines for the selection and application of algorithms that are best suited to particular study designs.

The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.;},

keywords={631/114/1305; 631/114/2415; 631/208/212; Agriculture; Amino Acid Sequence; Animal Genetics and Genomics; Animals; Artificial Intelligence; Base Sequence; Biomedicine; Cancer Research; Computational biology; Computer Simulation; Discriminant Analysis; Gene Function; Genetics, Medical; Genomics; Human Genetics; Humans; Innovations; Machine learning; Models, Genetic; Molecular Sequence Annotation; review-article; Technology application},

isbn={1471-0056;1471-0064;},

language={English},

}



@article{Ching_2018,

author={Ching,Travers and Himmelstein,Daniel S. and Beaulieu-Jones,Brett K. and Kalinin,Alexandr A. and Do,Brian T. and Way,Gregory P. and Ferrero,Enrico and Agapow,Paul-Michael and Zietz,Michael and Hoffman,Michael M. and Xie,Wei and Rosen,Gail L. and Lengerich,Benjamin J. and Israeli,Johnny and Lanchantin,Jack and Woloszynek,Stephen and Carpenter,Anne E. and Shrikumar,Avanti and Xu,Jinbo and Cofer,Evan M. and Lavender,Christopher A. and Turaga,Srinivas C. and Alexandari,Amr M. and Lu,Zhiyong and Harris,David J. and DeCaprio,Dave and Qi,Yanjun and Kundaje,Anshul and Peng,Yifan and Wiley,Laura K. and Segler,Marwin H. S. and Boca,Simina M. and Swamidass,S. J. and Huang,Austin and Gitter,Anthony and Greene,Casey S.},

year={2018},

title={Opportunities and obstacles for deep learning in biology and medicine},

journal={Journal of the Royal Society interface},

volume={15},

number={141},

pages={20170387-20170387},

abstract={Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems-patient classification, fundamental biological processes and treatment of patients-and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.;Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems-patient classification, fundamental biological processes and treatment of patients-and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems-patient classification, fundamental biological processes and treatment of patients-and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.;},

keywords={Algorithms; Biological activity; Biological effects; Biology; Biomedical Research - methods; Biomedical Research - trends; Biomedical Technology - trends; Decision Making; Deep Learning; Deep Learning - trends; Delivery of Health Care - methods; Delivery of Health Care - trends; Disease - genetics; Drug Design; Electronic Health Records - trends; Genomics; Headline Review; Humans; Learning algorithms; Literature reviews; Machine Learning; Medicine; Neural networks; Patients; Precision Medicine; Review; Review Articles; Terminology as Topic},

isbn={1742-5689;1742-5662;},

language={English},

}

@misc{Ghojogh_2019,
author={Ghojogh,Benyamin and Crowley,Mark},
year={2019},
title={Linear and Quadratic Discriminant Analysis: Tutorial},
journal={arXiv.org},
note={Copyright - © 2019. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License; Última actualización - 2019-06-08},
abstract={This tutorial explains Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) as two fundamental classification methods in statistical and probabilistic learning. We start with the optimization of decision boundary on which the posteriors are equal. Then, LDA and QDA are derived for binary and multiple classes. The estimation of parameters in LDA and QDA are also covered. Then, we explain how LDA and QDA are related to metric learning, kernel principal component analysis, Mahalanobis distance, logistic regression, Bayes optimal classifier, Gaussian naive Bayes, and likelihood ratio test. We also prove that LDA and Fisher discriminant analysis are equivalent. We finally clarify some of the theoretical concepts with simulations we provide.},
keywords={Business And Economics--Banking And Finance; Machine Learning; Statistical methods; Regression analysis; Discriminant analysis; Parameter estimation; Bayesian analysis; Learning; Principal components analysis; Probabilistic methods; Statistical analysis; Optimization; Likelihood ratio},
language={English},
url={http://www.espaciotv.es:2048/referer/secretcode/working-papers/linear-quadratic-discriminant-analysis-tutorial/docview/2236487209/se-2},
}

@article{Witten2011,

author={Witten,Daniela M. and Tibshirani,Robert},

year={2011},

title={Penalized classification using Fisher's linear discriminant},

journal={Journal of the Royal Statistical Society. Series B, Statistical methodology},

volume={73},

number={5},

pages={753-772},

abstract={We consider the supervised classification setting, in which the data consist of

p

features measured on

n

observations, each of which belongs to one of

K

classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where

p

≫

n

, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when

p

is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all

p

features. We propose

penalized LDA

, a general approach for penalizing the discriminant vectors in Fisher’s discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of

L

1

and fused lasso penalties. Our proposal is equivalent to recasting Fisher’s discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.;We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high dimensional setting where p≫n, LDA is not appropriate for two reasons. First, the standard estimate for the within‐class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule that is obtained from LDA, since it involves all p features. We propose penalized LDA, which is a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization–maximization approach to optimize it efficiently when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L1 and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high dimensional setting and explore their relationships with our proposal.;We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high dimensional setting where pn, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule that is obtained from LDA, since it involves all p features. We propose penalized LDA, which is a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach to optimize it efficiently when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L1 and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high dimensional setting and explore their relationships with our proposal. [PUBLICATION ABSTRACT];We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p ≫ n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.;We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p ≫ n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high-dimensional setting where p ≫ n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule obtained from LDA, since it involves all p features. We propose penalized LDA, a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach in order to efficiently optimize it when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L(1) and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high-dimensional setting, and explore their relationships with our proposal.;We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high dimensional setting where p‰«n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule that is obtained from LDA, since it involves all p features. We propose penalized LDA, which is a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach to optimize it efficiently when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L1 and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high dimensional setting and explore their relationships with our proposal.;We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high dimensional setting where p » n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule that is obtained from LDA, since it involves all p features. We propose penalized LDA, which is a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach to optimize it efficiently when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L₁ and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high dimensional setting and explore their relationships with our proposal.;We consider the supervised classification setting, in which the data consist of p features measured on n observations, each of which belongs to one of K classes. Linear discriminant analysis (LDA) is a classical method for this problem. However, in the high dimensional setting where p≫n, LDA is not appropriate for two reasons. First, the standard estimate for the within-class covariance matrix is singular, and so the usual discriminant rule cannot be applied. Second, when p is large, it is difficult to interpret the classification rule that is obtained from LDA, since it involves all p features. We propose penalized LDA, which is a general approach for penalizing the discriminant vectors in Fisher's discriminant problem in a way that leads to greater interpretability. The discriminant problem is not convex, so we use a minorization-maximization approach to optimize it efficiently when convex penalties are applied to the discriminant vectors. In particular, we consider the use of L1 and fused lasso penalties. Our proposal is equivalent to recasting Fisher's discriminant problem as a biconvex problem. We evaluate the performances of the resulting methods on a simulation study, and on three gene expression data sets. We also survey past methods for extending LDA to the high dimensional setting and explore their relationships with our proposal. Reprinted by permission of Blackwell Publishers;},

keywords={Centroids; Classification; covariance; Covariance matrices; data collection; Discriminant analysis; Discriminants; Eigenvectors; Estimation; Exact sciences and technology; Feature selection; Fines & penalties; Gene expression; General topics; Genes; High dimensional problems; Lasso; Linear discriminant analysis; Linear models; Markov processes; Mathematical vectors; Mathematics; Matrices; Measurement; Methodology; Multivariate analysis; Observation; Parametric inference; Principal components analysis; Probability and statistics; Probability theory and stochastic processes; Sciences and techniques of general use; Simulation; Statistical methods; Statistics; Studies; Supervised learning; Survey analysis},

isbn={1369-7412},

language={English},

}




@article{Clemmensen2011,

author={Clemmensen,Line and Hastie,Trevor and Witten,Daniela and Ersbøll,Bjarne},

year={2011},

title={Sparse Discriminant Analysis},

journal={Technometrics},

volume={53},

number={4},

pages={406-413},

abstract={We consider the problem of performing interpretable classification in the high-dimensional setting, in which the number of features is very large and the number of observations is limited. This setting has been studied extensively in the chemometrics literature, and more recently has become commonplace in biological and medical applications. In this setting, a traditional approach involves performing feature selection before classification. We propose sparse discriminant analysis, a method for performing linear discriminant analysis with a sparseness criterion imposed such that classification and feature selection are performed simultaneously. Sparse discriminant analysis is based on the optimal scoring interpretation of linear discriminant analysis, and can be extended to perform sparse discrimination via mixtures of Gaussians if boundaries between classes are nonlinear or if subgroups are present within each class. Our proposal also provides low-dimensional views of the discriminative directions.;We consider the problem of performing interpretable classification in the high-dimensional setting, in which the number of features is very large and the number of observations is limited. This setting has been studied extensively in the chemometrics literature, and more recently has become commonplace in biological and niedical applications. In this setting, a traditional approach involves performing feature selection before classification. We propose sparse discriminant analysis, a method for performing linear discriminant analysis with a sparseness criterion imposed such that classification and feature selection are performed simultaneously. Sparse discriminant analysis is based on the optimal scoring interpretation of linear discriminant analysis, and can be extended to perform sparse discrimination via mixtures of Gaussians if boundaries between classes are nonlinear or if subgroups are present within each class. Our proposal also provides low-dimensional views of the discriminative directions.;We consider the problem of performing interpretable classification in the high-dimensional setting, in which the number of features is very large and the number of observations is limited. This setting has been studied extensively in the chemometrics literature, and more recently has become commonplace in biological and medical applications. In this setting, a traditional approach involves performing feature selection before classification. We propose sparse discriminant analysis, a method for performing linear discriminant analysis with a sparseness criterion imposed such that classification and feature selection are performed simultaneously. Sparse discriminant analysis is based on the optimal scoring interpretation of linear discriminant analysis, and can be extended to perform sparse discrimination via mixtures of Gaussians if boundaries between classes are nonlinear or if subgroups are present within each class. Our proposal also provides low-dimensional views of the discriminative directions. [PUBLICATION ABSTRACT];},

keywords={Applied sciences; Centroids; Chemistry; Classification; Computer science; control theory; systems; Covariance matrices; Data processing. List processing. Character string processing; Decision theory. Utility theory; Dimension reduction; Discriminant analysis; Discriminants; Exact sciences and technology; Feature selection; General and physical chemistry; General. Nomenclature, chemical documentation, computer chemistry; Information classification; Least squares; Linear discriminant analysis; Linear inference, regression; Mathematical problems; Mathematical vectors; Mathematics; Matrices; Memory organisation. Data processing; Mixture discriminant analysis; Normal distribution; Operational research and scientific management; Operational research. Management science; Probability and statistics; Sciences and techniques of general use; Silhouettes; Software; Sparsity; Statistics; Theory of reactions, general kinetics. Catalysis. Nomenclature, chemical documentation, computer chemistry},

isbn={0040-1706},

language={English},

}



@article{Hastie1986,

author={Hastie,Trevor and Tibshirani,Robert},

year={1986},

title={Generalized Additive Models},

journal={Statistical science},

volume={1},

number={3},

pages={297-310},

abstract={Likelihood-based regression models such as the normal linear regression model and the linear logistic model, assume a linear (or some other parametric) form for the covariates X_1, X_2, \cdots, X_p. We introduce the class of generalized additive models which replaces the linear form \sum \beta_jX_j by a sum of smooth functions \sum s_j(X_j). The s_j(\cdot)'s are unspecified functions that are estimated using a scatterplot smoother, in an iterative procedure we call the local scoring algorithm. The technique is applicable to any likelihood-based regression model: the class of generalized linear models contains many of these. In this class the linear predictor \eta = \Sigma \beta_jX_j is replaced by the additive predictor \Sigma s_j(X_j); hence, the name generalized additive models. We illustrate the technique with binary response and survival data. In both cases, the method proves to be useful in uncovering nonlinear covariate effects. It has the advantage of being completely automatic, i.e., no "detective work" is needed on the part of the statistician. As a theoretical underpinning, the technique is viewed as an empirical method of maximizing the expected log likelihood, or equivalently, of minimizing the Kullback-Leibler distance to the true model.;Likelihood-based regression models such as the normal linear regression model and the linear logistic model, assume a linear (or some other parametric) form for the covariates X1, X2, ⋯, Xp. We introduce the class of generalized additive models which replaces the linear form ∑ βjXj by a sum of smooth functions ∑ sj(Xj). The sj(·)'s are unspecified functions that are estimated using a scatterplot smoother, in an iterative procedure we call the local scoring algorithm. The technique is applicable to any likelihood-based regression model: the class of generalized linear models contains many of these. In this class the linear predictor η = Σ βjXj is replaced by the additive predictor Σ sj(Xj); hence, the name generalized additive models. We illustrate the technique with binary response and survival data. In both cases, the method proves to be useful in uncovering nonlinear covariate effects. It has the advantage of being completely automatic, i.e., no "detective work" is needed on the part of the statistician. As a theoretical underpinning, the technique is viewed as an empirical method of maximizing the expected log likelihood, or equivalently, of minimizing the Kullback-Leibler distance to the true model.;},

keywords={Data smoothing; Estimation methods; Generalized linear model; Generalized linear models; Least squares; Linear regression; Logistics; Maximum likelihood estimation; Modeling; nonlinearity; nonparametric regression; Parametric models; partial residuals; Regression analysis; smoothing},

isbn={0883-4237},

language={English},

}


@article{Maron1961,
  title={Automatic indexing: an experimental inquiry},
  author={Maron, Melvin Earl},
  journal={Journal of the ACM (JACM)},
  volume={8},
  number={3},
  pages={404--417},
  year={1961},
  publisher={ACM New York, NY, USA},
}

@inproceedings{rish2001,
  title={An empirical study of the naive Bayes classifier},
  author={Rish, Irina},
  booktitle={Proceedings of the IJCAI 2001 Workshop on Empirical Methods in Artificial Intelligence},
  pages={41--46},
  year={2001},
  address={Seattle, USA},
}


@article{dudoit2002,
  title={Comparison of discrimination methods for the classification of tumors using gene expression data},
  author={Dudoit, Sandrine and Fridlyand, Jane and Speed, Terence P},
  journal={Journal of the American Statistical Association},
  volume={97},
  number={457},
  pages={77--87},
  year={2002},
  publisher={Taylor \& Francis},
  doi={10.1198/016214502753479248},
}


@article{Saglam2017,
  title={Sequential image segmentation based on minimum spanning tree representation},
  author={Saglam, Ali and Baykan, Nurdan Akhan},
  journal={Pattern Recognition Letters},
  volume={87},
  pages={155--162},
  year={2017},
  publisher={Elsevier},
}


@article{Breiman2001,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}



@article{Ishwaran2008,
  title={Random survival forests},
  author={Ishwaran, Hemant and Kogalur, Udaya B and Blackstone, Eugene H and Lauer, Michael S},
  year={2008}
}

@incollection{qi2012,
  title={Random forest for bioinformatics},
  author={Qi, Yanjun},
  booktitle={Ensemble machine learning},
  pages={307--323},
  year={2012},
  publisher={Springer}
}

@article{cortes1995,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{suykens1999,
  title={Least squares support vector machine classifiers},
  author={Suykens, Johan AK and Vandewalle, Joos},
  journal={Neural processing letters},
  volume={9},
  number={3},
  pages={293--300},
  year={1999},
  publisher={Springer}
}

@inproceedings{veropoulos1999,
  title={Controlling the sensitivity of support vector machines},
  author={Veropoulos, Konstantinos and Campbell, Colin and Cristianini, Nello and others},
  booktitle={Proceedings of the international joint conference on AI},
  volume={55},
  pages={60},
  year={1999},
  organization={Stockholm}
}

@article{khemchandani2007twin,
  title={Twin support vector machines for pattern classification},
  author={Khemchandani, Reshma and Chandra, Suresh and others},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={29},
  number={5},
  pages={905--910},
  year={2007},
  publisher={IEEE}
}

@article{khemchandani2007,
  title={Twin support vector machines for pattern classification},
  author={Khemchandani, Reshma and Chandra, Suresh and others},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={29},
  number={5},
  pages={905--910},
  year={2007},
  publisher={IEEE}
}


@article{lisboa2002,
  title={A review of evidence of health benefit from artificial neural networks in medical intervention},
  author={Lisboa, Paulo JG},
  journal={Neural networks},
  volume={15},
  number={1},
  pages={11--39},
  year={2002},
  publisher={Elsevier}
}



@article{gurovich2019,
  title={Identifying facial phenotypes of genetic disorders using deep learning},
  author={Gurovich, Yaron and Hanani, Yair and Bar, Omri and Nadav, Guy and Fleischer, Nicole and Gelbman, Dekel and Basel-Salmon, Lina and Krawitz, Peter M and Kamphausen, Susanne B and Zenker, Martin and others},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={60--64},
  year={2019},
  publisher={Nature Publishing Group US New York}
}


@article{topol2019,
  title={High-performance medicine: the convergence of human and artificial intelligence},
  author={Topol, Eric J},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={44--56},
  year={2019},
  publisher={Nature Publishing Group US New York}
}


@article{com2021laying,
  title={Laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts},
  author={COM, EU},
  journal={Proposal for a regulation of the European parliament and of the council},
  year={2021}
}
@misc{FDA_AI_SaMD_2025,
  author       = {{U.S. Food and Drug Administration}},
  title        = {Artificial Intelligence in Software as a Medical Device},
  year         = {2025},
  month        = mar,
  day          = {25},
  howpublished = {\url{https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-software-medical-device}},
  note         = {Contenido actualizado al 25 de marzo de 2025}
}

@misc{EC_AI_Act_2025,
  author       = {{Comisión Europea}},
  title        = {Ley de IA (reglamento sobre inteligencia artificial)},
  year         = {2025},
  month        = aug,
  howpublished = {\url{https://digital-strategy.ec.europa.eu/es/policies/regulatory-framework-ai}},
  note         = {Versión en español del portal de estrategia digital, actualizada al 1 de agosto de 2025}
}